{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xJrLAd2Z7rUO",
    "outputId": "2997436d-0c8d-4719-ac19-bdca3edee085"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets tokenizers seqeval -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wye4Zxdm-yEx",
    "outputId": "e2e98253-d311-412f-add2-87655ce8a464"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N6mpYGiO8Rme",
    "outputId": "abace6a6-c5b2-4d22-8d21-2ebe80f01552"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install evaluate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "id": "Ya2CgZw5uAaH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizerFast, BertModel, TrainingArguments, Trainer, DataCollatorForTokenClassification, AutoModelForTokenClassification\n",
    "import datasets\n",
    "import evaluate\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 379,
     "referenced_widgets": [
      "0800ef7f480441a3a0ceee718917e3a7",
      "8316a100865d4732ab8fbfa7fffa398e",
      "34f0922847ce423ca273c126dcf62b6b",
      "306fc5b6c1b84184ba46e1ebf3eba792",
      "984559a53e6a4c33b56859631c4a26ac",
      "39e0a0cb8c5747d3b966548358cd7598",
      "614009c08f094964833ac9376810aaf6",
      "2c019fda88e049c39301add03ed745bb",
      "9782a07454664500a3ca4a4f469c6f34",
      "a68e4980f32342669711b10eb8510124",
      "80eb4c33b9a94e3c82fe8e26abe3fd20",
      "2304f691e2574c71a9c344f4d520078c",
      "4f264094c0e54776a7c72036604d577a",
      "ced6971e5fd64d0d87354acc421e991d",
      "7ebbac6d56584e1fb4a73afab89d093f",
      "bff4dbee504f4abd9f29182dca72f81c",
      "0c363ea102034592863eb3d9be9e18fc",
      "5c43e70093704a85a6be91deb34f00a1",
      "4a0818c85e6c41b589bfbd2148dddfc4",
      "4a668d280f3b4c159bc4e1f23b1f6651",
      "4d094ce5cf0b488795487ce706eba766",
      "9fc829c1d10f434f9d7d9a8cfad5598a",
      "14e648bfe9d2459ea5306f1d626f3297",
      "ab4b2bfa192840749284dd3f71987521",
      "abbc87b49a95432697302fb6a619d04a",
      "98d3c82aa818497f9ff786587db22330",
      "748f6b5e17304679a3c490d55c26fd62",
      "2f1318a817e044c895ec595fe4c72125",
      "1464713919d3474396625f466b0c83f4",
      "d35573e7b2114776b9a1ba8465586732",
      "a3f3d08e444d4a8b91831eae369af346",
      "f110675aba434fd890e44973c2d2dad0",
      "fee1b9d467cb4c3e93026744b359e0a6",
      "4ee1d092639d4de3b3264ebe844943db",
      "978381158b9e4a35a529450413a5ba37",
      "67f3eb00088f4fc5bd5cc06048ffd506",
      "e2a570804a5d44389fc15ffd7fe6e18e",
      "8f9e64cb5de245e9a389a003f4d4b929",
      "7e7093f8819146968fbf1f34ecdfa3df",
      "5b414f6259d746ea86592be90bdc8481",
      "a974b8e1cb4d4788bc89ced7e28e442b",
      "edaa22e13c29429f88bde6af5eb1e633",
      "6e4859e62032490d87bf82f2be861136",
      "123958383a1c4741a99905672b75fe66",
      "b547db9e983f40e78d1dc71b978ecc36",
      "f8b98a24ba874ef8aad02447d2a7dcf8",
      "c403fa8235e4486aa16b3ae7bc5b2d95",
      "dc4043e1867e4bce8807da2d1e21f1a5",
      "967a90b46d1a4551abf83e12b1dbdbbd",
      "46bcddf75d9b404dac6fb798ab896a9d",
      "8ae9f35a94dd4be4be4970e2d4d080b6",
      "76893be392544cd5b3196adcc0e3f286",
      "778a1ce97b17420a9df865f2a7c3bd99",
      "3396783e12084bd2b3ca17bf839a7f83",
      "b9baa962e95b4567b68b2cb85bcca808",
      "99e439caf9254229934080d83669ae92",
      "d14c95b38e6a436f89922b23af750eba",
      "1e9f312111a94055852b3581f96258b3",
      "c3c91a5c7ba14700b2c6a0c9575a589f",
      "0c3dbbce73924cf4b19adcd2665accfb",
      "609f3ef6bd3d4f7e94d95d506cb0623b",
      "caa84393e23d490680b04e11652e0bb3",
      "9271556699874012a5435d02bdc7ccd3",
      "049182790d7e40b09c0316a18543cb47",
      "8faef5267acc4b62b107e97678196c46",
      "201c36b1ca4a44eda1fdfccff573b660"
     ]
    },
    "id": "cnrgJkt6xF3s",
    "outputId": "f19cac13-3b28-49e3-cd96-95df2e4694b8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0800ef7f480441a3a0ceee718917e3a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/12.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2304f691e2574c71a9c344f4d520078c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "conll2003.py:   0%|          | 0.00/9.57k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The repository for conll2003 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/conll2003.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N] y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e648bfe9d2459ea5306f1d626f3297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/983k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ee1d092639d4de3b3264ebe844943db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b547db9e983f40e78d1dc71b978ecc36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e439caf9254229934080d83669ae92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Loading Dataset and Labels\n",
    "\n",
    "dataset = datasets.load_dataset(\"conll2003\")\n",
    "\n",
    "# Get the list of label names for each task.\n",
    "ner_label_list = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "pos_label_list = dataset[\"train\"].features[\"pos_tags\"].feature.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "a546a81a0857475db8e3adceaebcc849",
      "eca6650977d84cc6b231fa652bce1e8d",
      "b931e848f451479cb011090dbe76688c",
      "d80e796380544e16affa90119a28c0ac",
      "f6b16386a871426583626d8874777922",
      "ad23015def1440d8b08547023cebbf20",
      "da4767bb5a964fc8985659babd84d10a",
      "970c4de5c7b6440eb689d903aa6de778",
      "344d967d8cce496d83af40e83bb31ba9",
      "a1258717b75c47edbd6c56364ba2596e",
      "3d25658e648942b1b0c7d1804080f4ca",
      "953492c449aa48a695dbed959ec37981",
      "367ff348e61642d5917a8ec2fa5d6993",
      "d8820cd724bb4850bb4524ba60e797c6",
      "17e40e44fbd54399a9b857ffda0ae79c",
      "55033d31f19643b49415debdeab19a87",
      "17d8805542d94beda4e767e242814ce7",
      "ffcd727482a244c38be58103f5b1a1f5",
      "0c85a548c4144b24b885d2b5e44533d4",
      "2269b227886c466794282c81943010a0",
      "67b30cd1774e48528d5a8e88ef97698d",
      "8348a8c3230b4c4eba4f4e829efa4cd7",
      "6ad857f44f2c43df8c03741c9de9cb3b",
      "2b705afe892444f89590e7042717bd68",
      "e90d81638b87415d9374efc6f70ff3d5",
      "f4ccd5cfd9164ee7bae2a2a3b84e4fe0",
      "be9d5ab6adef4b969206b6502c020278",
      "90d72c527f9b48f0901c906af5238ca2",
      "fe3cc97ca59c4be08a54dc50a1c36058",
      "f995f529828a442bb4240fdf40dbe2db",
      "93fa143fb85641afae9f6e0fcdef508f",
      "44e82eae9ea04755a3169f2c73d02e4c",
      "63fff23db0494685810631e222bd10bc",
      "1e53d45678794a85b572f7817451829f",
      "627efb1745bb4798b9f589d89371877f",
      "6cff9bfd09734c8682aa21c883660316",
      "487b70955c9c4848929b23a5f7e8c9fc",
      "75720a9fa4004da6bcaa25234efa9e93",
      "266427b77a6b4518af2b0af02ccec543",
      "1a1eeebd57d4485986bf901d237f6d9c",
      "840979624f8b452eab4f9de9a9e8b5ae",
      "e14c9e94674341179def5a6093f82444",
      "845bb8a93f9e4c1ca5c1c8961c133b31",
      "9aa779b449e84ded849aa38b272d5e31"
     ]
    },
    "id": "RwkSGfkkxIo1",
    "outputId": "76b4f1da-26cd-4f82-ff65-0d89aafb8a76"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a546a81a0857475db8e3adceaebcc849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "953492c449aa48a695dbed959ec37981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ad857f44f2c43df8c03741c9de9cb3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e53d45678794a85b572f7817451829f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. Initialize Tokenizer\n",
    "\n",
    "# Using the fast BERT tokenizer (cased) so that pre-tokenized words are handled correctly.\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "a3a49e20e6ba4a3ea56e41f71b6de9ad",
      "7381da9141c847a48d2fe3bef9564c07",
      "d340a3c81659437b972a1950fe4fd4da",
      "1c231873234b489d82ce0e9574887b9d",
      "ddf5143987794eeba6db2ccf52908377",
      "01519748c71c42ffa2771bc1a9e6e893",
      "502d2380552846cabfabed08846ef820",
      "13cb1b7fad6f4c408d2d2e194a263ce2",
      "850b9c1770054bbd916897d6af146622",
      "ab98fb77942f4fa88c6e6845f9935e0e",
      "323c7c7b08d4424fbfc39c427b53f03b",
      "350182563db14bff9b8127e7006cbe51",
      "5987b764f8014f9e850269c89492a64e",
      "5dbe7a95fd57402c9ae7901cbb081c74",
      "5a2aaa80939b4180909a5a57fb154058",
      "fdfeb81a944540a9a25453e0107d27d4",
      "8422697698384b3bb56c8f885ade3a5c",
      "4ee1f823bad64130a905ba6c25560f3d",
      "f9afe0e2720e4481b51421b1d9842023",
      "2e2389f9329840049aeace277a044341",
      "5c8023791b2744baaf82909b1a6fe392",
      "6523047d29fc47b0849e24a9c31bdebc",
      "ef3f63e311784663aaeb2e835190259b",
      "466ee7776bd74ec58f47f95bf4648c59",
      "3cecd5843c2c4707bef415bf96e4d357",
      "c3cd6ed3aab342bc94eacd7407856f96",
      "5f43cc850b994967861f16be66f4fd21",
      "a7d673b3a28749068a53b165dae754d7",
      "a24fdb00edaf4ce796ce8e159cad6e0d",
      "d55405540a1f44628b355295e48fd9a4",
      "72803cd4aa2044f684b6cd2b2c7c99b9",
      "b4b7ce40e9ac42679dc60174e7be1bbc",
      "d62ba37509ea49339fbf6d52ed4b95a0"
     ]
    },
    "id": "JZLNnr5fxKeD",
    "outputId": "b83caab8-0fbb-45bf-9118-e43a9687720d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3a49e20e6ba4a3ea56e41f71b6de9ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "350182563db14bff9b8127e7006cbe51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef3f63e311784663aaeb2e835190259b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_and_align_labels(examples, label_all_tokens=True):\n",
    "    \"\"\"\n",
    "    Tokenizes inputs while aligning word-level NER and POS labels to (possibly split) tokens.\n",
    "    Subword tokens are either given the same label (if label_all_tokens=True) or masked with -100.\n",
    "    \"\"\"\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"],\n",
    "                                 truncation=True,\n",
    "                                 is_split_into_words=True)\n",
    "\n",
    "    all_ner_labels = []\n",
    "    all_pos_labels = []\n",
    "\n",
    "    # Iterate over the batch using the number of examples in input_ids.\n",
    "    for i in range(len(tokenized_inputs[\"input_ids\"])):\n",
    "        # Get the mapping from token indices to original word indices for the i-th example.\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        ner_label_ids = []\n",
    "        pos_label_ids = []\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            # When word_idx is None, it is a special token (e.g., CLS or SEP)\n",
    "            if word_idx is None:\n",
    "                ner_label_ids.append(-100)\n",
    "                pos_label_ids.append(-100)\n",
    "            # For the first token of a given word, assign the real label.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                ner_label_ids.append(examples[\"ner_tags\"][i][word_idx])\n",
    "                pos_label_ids.append(examples[\"pos_tags\"][i][word_idx])\n",
    "            # For subsequent tokens of the same word:\n",
    "            else:\n",
    "                ner_label_ids.append(examples[\"ner_tags\"][i][word_idx] if label_all_tokens else -100)\n",
    "                pos_label_ids.append(examples[\"pos_tags\"][i][word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        all_ner_labels.append(ner_label_ids)\n",
    "        all_pos_labels.append(pos_label_ids)\n",
    "\n",
    "    tokenized_inputs[\"ner_labels\"] = all_ner_labels\n",
    "    tokenized_inputs[\"pos_labels\"] = all_pos_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Apply the tokenization function to the entire dataset.\n",
    "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103,
     "referenced_widgets": [
      "b110b5e6b6a044398d28da46e64e3731",
      "9dd92bbe7641406a9f2afed3ca3cb9ed",
      "42b6cf713422466b9d947a3f140b34de",
      "6c69881d74f349559eda08774a101357",
      "71f930b914214b83945fbfcc42cf7274",
      "82ad2a0fb1b24687baff42e266d25fd1",
      "e13f0b20b2e24cf8b3204645b8b785a3",
      "4369ac066bb24244bc72aef89f502e65",
      "a78a879f54ac46c78649e42f9efbb6d8",
      "678582ca58b84aa78165acbb5e81bb32",
      "1a1dc4bc79ce43aab56e056dd73e4a08"
     ]
    },
    "id": "19-vnpVUxNbY",
    "outputId": "e17e992a-9735-488a-c405-0fb1f88c6fb1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b110b5e6b6a044398d28da46e64e3731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##############################################\n",
    "# 4. Define the Multi-Task Model\n",
    "##############################################\n",
    "\n",
    "class MultiTaskBertForTokenClassification(nn.Module):\n",
    "    \"\"\"\n",
    "    A custom model that extends BERT with two token classification heads:\n",
    "      - One for NER (e.g., PER, LOC, ORG, MISC)\n",
    "      - One for POS tagging (e.g., nouns, verbs, adjectives, etc.)\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name, num_ner_labels, num_pos_labels):\n",
    "        super(MultiTaskBertForTokenClassification, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.ner_classifier = nn.Linear(self.bert.config.hidden_size, num_ner_labels)\n",
    "        self.pos_classifier = nn.Linear(self.bert.config.hidden_size, num_pos_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, ner_labels=None, pos_labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "\n",
    "        ner_logits = self.ner_classifier(sequence_output)\n",
    "        pos_logits = self.pos_classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if ner_labels is not None and pos_labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "            # Compute loss for each task independently, then sum them.\n",
    "            ner_loss = loss_fct(ner_logits.view(-1, ner_logits.shape[-1]), ner_labels.view(-1))\n",
    "            pos_loss = loss_fct(pos_logits.view(-1, pos_logits.shape[-1]), pos_labels.view(-1))\n",
    "            loss = ner_loss + pos_loss\n",
    "\n",
    "        return {\"loss\": loss, \"ner_logits\": ner_logits, \"pos_logits\": pos_logits}\n",
    "\n",
    "# Instantiate the multi-task model using the number of labels for each task.\n",
    "num_ner_labels = len(ner_label_list)\n",
    "num_pos_labels = len(pos_label_list)\n",
    "model = MultiTaskBertForTokenClassification(\"bert-base-cased\", num_ner_labels, num_pos_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1PwzlA5MxQrO"
   },
   "outputs": [],
   "source": [
    "##############################################\n",
    "# 5. Create a Custom Data Collator (Fixed Version)\n",
    "##############################################\n",
    "\n",
    "def data_collator(features):\n",
    "    \"\"\"\n",
    "    Pads the inputs as well as the ner_labels and pos_labels.\n",
    "\n",
    "    This version pops the labels from each example so the tokenizer.pad method\n",
    "    only handles the standard fields (like input_ids, attention_mask, etc.). The labels\n",
    "    are then padded manually.\n",
    "    \"\"\"\n",
    "    # Extract label fields and remove them from the features dict.\n",
    "    ner_labels = [f.pop(\"ner_labels\") for f in features]\n",
    "    pos_labels = [f.pop(\"pos_labels\") for f in features]\n",
    "\n",
    "    # Use the tokenizer to pad the remaining fields.\n",
    "    batch = tokenizer.pad(features, return_tensors=\"pt\")\n",
    "\n",
    "    # Manually pad the label sequences using -100 as the padding index.\n",
    "    batch[\"ner_labels\"] = torch.nn.utils.rnn.pad_sequence(\n",
    "        [torch.tensor(label_seq) for label_seq in ner_labels],\n",
    "        batch_first=True,\n",
    "        padding_value=-100,\n",
    "    )\n",
    "    batch[\"pos_labels\"] = torch.nn.utils.rnn.pad_sequence(\n",
    "        [torch.tensor(label_seq) for label_seq in pos_labels],\n",
    "        batch_first=True,\n",
    "        padding_value=-100,\n",
    "    )\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "2b621d4c37104389b7fe5a426d126098",
      "0427efd8148e4503b6accdc2d09f6bef",
      "4ec160054e6140a6bc7d20643a803235",
      "e10ff0bcada149b1ba86718e83868761",
      "cdfb018b8322444b83ff742a76daee07",
      "61f4b15de0c64a508f9b840baf43b89e",
      "6a20b20c6ee3451a8608db1916ed2c83",
      "04a06aa21f094ef3801f331fc7753dde",
      "c425e4c0a61143ec9f91195424217340",
      "7f5b25751fc8474583e5c9d3e417e187",
      "59288cbbc2f84825acb1f5d0c38e458c"
     ]
    },
    "id": "o3ZWpDtSxS7d",
    "outputId": "d38d38a6-0aec-41d3-caaa-a3f777b6811d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b621d4c37104389b7fe5a426d126098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 6. Define Evaluation Metrics\n",
    "\n",
    "# Load the seqeval metric for NER evaluation.\n",
    "ner_metric = evaluate.load(\"seqeval\") # seqeval is usde for sequence labelling\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Computes metrics for both tasks:\n",
    "      - For NER: uses the seqeval metric (precision, recall, F1)\n",
    "      - For POS: calculates a simple token-level accuracy (ignoring padding tokens)\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    ner_logits = predictions[\"ner_logits\"]\n",
    "    pos_logits = predictions[\"pos_logits\"]\n",
    "    ner_labels = labels[\"ner_labels\"]\n",
    "    pos_labels = labels[\"pos_labels\"]\n",
    "\n",
    "    # Convert logits to predicted label IDs.\n",
    "    ner_pred_ids = np.argmax(ner_logits, axis=-1)\n",
    "    pos_pred_ids = np.argmax(pos_logits, axis=-1)\n",
    "\n",
    "    true_ner_labels = []\n",
    "    pred_ner_labels = []\n",
    "    true_pos_labels = []\n",
    "    pred_pos_labels = []\n",
    "\n",
    "    # Loop through each sentence to remove padding (-100) and map indices to label names.\n",
    "    for i in range(len(ner_labels)):\n",
    "        ner_true = []\n",
    "        ner_pred = []\n",
    "        pos_true = []\n",
    "        pos_pred = []\n",
    "        for j in range(len(ner_labels[i])):\n",
    "            if ner_labels[i][j] != -100:\n",
    "                ner_true.append(ner_label_list[ner_labels[i][j]])\n",
    "                ner_pred.append(ner_label_list[ner_pred_ids[i][j]])\n",
    "            if pos_labels[i][j] != -100:\n",
    "                pos_true.append(pos_label_list[pos_labels[i][j]])\n",
    "                pos_pred.append(pos_label_list[pos_pred_ids[i][j]])\n",
    "        true_ner_labels.append(ner_true)\n",
    "        pred_ner_labels.append(ner_pred)\n",
    "        true_pos_labels.append(pos_true)\n",
    "        pred_pos_labels.append(pos_pred)\n",
    "\n",
    "    # Compute NER metrics.\n",
    "    ner_results = ner_metric.compute(predictions=pred_ner_labels, references=true_ner_labels)\n",
    "\n",
    "    # Compute POS token-level accuracy.\n",
    "    total_tokens = 0\n",
    "    correct_tokens = 0\n",
    "    for true_seq, pred_seq in zip(true_pos_labels, pred_pos_labels):\n",
    "        for t, p in zip(true_seq, pred_seq):\n",
    "            total_tokens += 1\n",
    "            if t == p:\n",
    "                correct_tokens += 1\n",
    "    pos_accuracy = correct_tokens / total_tokens if total_tokens > 0 else 0.0\n",
    "\n",
    "    return {\"ner\": ner_results, \"pos_accuracy\": pos_accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EShGQlE1xVm0",
    "outputId": "a3495b3b-9bd2-4689-bd87-8e3709611f81"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-b11c8d3aa49f>:12: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# 7. Set Up the Trainer and Training Arguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"multi_task_ner_pos\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "08O_NJHRxXhw",
    "outputId": "269f1bba-af40-4468-a8ef-28db19f90071"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ··········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkartik-c612\u001b[0m (\u001b[33mkartik-c612-university-of-british-columbia\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250424_130937-r54ibtwb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kartik-c612-university-of-british-columbia/huggingface/runs/r54ibtwb' target=\"_blank\">multi_task_ner_pos</a></strong> to <a href='https://wandb.ai/kartik-c612-university-of-british-columbia/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kartik-c612-university-of-british-columbia/huggingface' target=\"_blank\">https://wandb.ai/kartik-c612-university-of-british-columbia/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kartik-c612-university-of-british-columbia/huggingface/runs/r54ibtwb' target=\"_blank\">https://wandb.ai/kartik-c612-university-of-british-columbia/huggingface/runs/r54ibtwb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2634' max='2634' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2634/2634 08:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.064200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.408300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.320600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.278100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.246400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2634, training_loss=0.45164414943848585, metrics={'train_runtime': 783.3838, 'train_samples_per_second': 53.771, 'train_steps_per_second': 3.362, 'total_flos': 0.0, 'train_loss': 0.45164414943848585, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8. Training\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "H_nA_sb2xZd9"
   },
   "outputs": [],
   "source": [
    "# 9. Save the Model and Update Configuration\n",
    "\n",
    "import os\n",
    "\n",
    "model_save_path = \"multi_task_ner_pos_model\"\n",
    "os.makedirs(model_save_path, exist_ok=True)  # Create directory if it does not exist\n",
    "\n",
    "\n",
    "# Save model weights and tokenizer\n",
    "torch.save(model.state_dict(), model_save_path + \"/pytorch_model.bin\")\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "# Save additional configuration details (such as label mappings) into the config.\n",
    "config = model.bert.config.to_dict()\n",
    "config[\"id2label_ner\"] = {i: label for i, label in enumerate(ner_label_list)}\n",
    "config[\"label2id_ner\"] = {label: i for i, label in enumerate(ner_label_list)}\n",
    "config[\"id2label_pos\"] = {i: label for i, label in enumerate(pos_label_list)}\n",
    "config[\"label2id_pos\"] = {label: i for i, label in enumerate(pos_label_list)}\n",
    "with open(os.path.join(model_save_path, \"config.json\"), \"w\") as f:\n",
    "    json.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y7IMkWzmxvyY",
    "outputId": "bbf56db5-bcd4-4e14-d15e-8b8fc49c16fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Output:\n",
      "[{'entity': 'B-PER', 'word': 'Bill'}, {'entity': 'I-PER', 'word': 'Gates'}, {'entity': 'O', 'word': 'is'}, {'entity': 'O', 'word': 'the'}, {'entity': 'O', 'word': 'Founder'}, {'entity': 'O', 'word': 'of'}, {'entity': 'B-ORG', 'word': 'Microsoft'}]\n",
      "\n",
      "POS Output:\n",
      "[{'token': 'Bill', 'pos': 'NNP'}, {'token': 'Gates', 'pos': 'NNP'}, {'token': 'is', 'pos': 'VBZ'}, {'token': 'the', 'pos': 'DT'}, {'token': 'Founder', 'pos': 'NNP'}, {'token': 'of', 'pos': 'IN'}, {'token': 'Microsoft', 'pos': 'NNP'}]\n"
     ]
    }
   ],
   "source": [
    "def run_inference(text):\n",
    "    \"\"\"\n",
    "    Runs the multi-task model on an input text string and returns both NER and POS predictions.\n",
    "    The function tokenizes the input and aligns the predictions based on the word_ids.\n",
    "    \"\"\"\n",
    "    # Determine the device the model is on (or move it to one if you prefer)\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Tokenize the input text.\n",
    "    tokenized_input = tokenizer(text, return_tensors=\"pt\", is_split_into_words=False)\n",
    "\n",
    "    # Move tokenized tensors to the same device as the model.\n",
    "    tokenized_input = {k: v.to(device) for k, v in tokenized_input.items()}\n",
    "\n",
    "    # Obtain the model outputs.\n",
    "    outputs = model(**tokenized_input)\n",
    "    ner_logits = outputs[\"ner_logits\"].detach().cpu().numpy()\n",
    "    pos_logits = outputs[\"pos_logits\"].detach().cpu().numpy()\n",
    "\n",
    "    # Get predicted label IDs.\n",
    "    ner_pred_ids = np.argmax(ner_logits, axis=-1)[0]\n",
    "    pos_pred_ids = np.argmax(pos_logits, axis=-1)[0]\n",
    "\n",
    "    # Convert input ids to tokens.\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"].detach().cpu()[0])\n",
    "\n",
    "    # Retrieve the word ids for alignment.\n",
    "    word_ids = tokenizer(text, return_tensors=\"pt\", is_split_into_words=False).word_ids(batch_index=0)\n",
    "\n",
    "    # Aggregate predictions for each original word.\n",
    "    ner_results = []\n",
    "    pos_results = []\n",
    "    last_word = None\n",
    "    for idx, word_idx in enumerate(word_ids):\n",
    "        if word_idx is None:\n",
    "            continue\n",
    "        # To avoid repeating predictions for subword tokens, only record the first token for each word.\n",
    "        if word_idx != last_word:\n",
    "            ner_results.append({\n",
    "                \"entity\": ner_label_list[ner_pred_ids[idx]],\n",
    "                \"word\": tokens[idx]\n",
    "            })\n",
    "            pos_results.append({\n",
    "                \"token\": tokens[idx],\n",
    "                \"pos\": pos_label_list[pos_pred_ids[idx]]\n",
    "            })\n",
    "            last_word = word_idx\n",
    "    return ner_results, pos_results\n",
    "\n",
    "# Example usage similar to the original NER pipeline demonstration.\n",
    "example = \"Bill Gates is the Founder of Microsoft\"\n",
    "ner_out, pos_out = run_inference(example)\n",
    "print(\"NER Output:\")\n",
    "print(ner_out)\n",
    "print(\"\\nPOS Output:\")\n",
    "print(pos_out)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
