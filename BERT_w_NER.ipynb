{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOJZqDRP6JHrTMsh8EoQUKv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/c-kartik/BERT_NER/blob/main/BERT_w_NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets tokenizers seqeval -q"
      ],
      "metadata": {
        "collapsed": true,
        "id": "BzHLnSPT_wkC"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "import transformers\n",
        "import numpy as np\n",
        "from transformers import BertTokenizerFast, DataCollatorForTokenClassification, AutoModelForTokenClassification\n",
        "\n",
        "conll2003 = datasets.load_dataset(\"conll2003\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mrpiNhNwbQqg"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conll2003"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dvLcah8wcU4R",
        "outputId": "fd492f64-6eba-4fc8-cb0b-4cfa14c88a35"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 14041\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 3250\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 3453\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conll2003.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TUvSaNvfdT0w",
        "outputId": "532ff320-1a94-4a5d-ae8c-3fbdd79f4722"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': (14041, 5), 'validation': (3250, 5), 'test': (3453, 5)}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conll2003[\"train\"][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XAZ3IovrdYba",
        "outputId": "a1b33b6e-460c-4078-cb85-33a93f40355f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '0',\n",
              " 'tokens': ['EU',\n",
              "  'rejects',\n",
              "  'German',\n",
              "  'call',\n",
              "  'to',\n",
              "  'boycott',\n",
              "  'British',\n",
              "  'lamb',\n",
              "  '.'],\n",
              " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
              " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
              " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conll2003[\"train\"].features['ner_tags']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "70TgEXj_dYYj",
        "outputId": "c882a8f7-78a4-4368-d3db-ab1f9cdd63e5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conll2003[\"train\"].description"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "collapsed": true,
        "id": "f3zErblpdYVr",
        "outputId": "1ebd4d33-2c8d-4a40-9a05-9f31dfbbc513"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The shared task of CoNLL-2003 concerns language-independent named entity recognition. We will concentrate on\\nfour types of named entities: persons, locations, organizations and names of miscellaneous entities that do\\nnot belong to the previous three groups.\\n\\nThe CoNLL-2003 shared task data files contain four columns separated by a single space. Each word has been put on\\na separate line and there is an empty line after each sentence. The first item on each line is a word, the second\\na part-of-speech (POS) tag, the third a syntactic chunk tag and the fourth the named entity tag. The chunk tags\\nand the named entity tags have the format I-TYPE which means that the word is inside a phrase of type TYPE. Only\\nif two phrases of the same type immediately follow each other, the first word of the second phrase will have tag\\nB-TYPE to show that it starts a new phrase. A word with tag O is not part of a phrase. Note the dataset uses IOB2\\ntagging scheme, whereas the original dataset uses IOB1.\\n\\nFor more details see https://www.clips.uantwerpen.be/conll2003/ner/ and https://www.aclweb.org/anthology/W03-0419\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "TIZWNtvSdYSk"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = conll2003['train'][0]\n",
        "\n",
        "tokenized_input = tokenizer(example_text['tokens'], is_split_into_words=True)\n",
        "tokenized_input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "o0OymwDadYPx",
        "outputId": "b9fed753-b0f8-4721-f023-b7f23d707886"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 7270, 22961, 1528, 1840, 1106, 21423, 1418, 2495, 12913, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_input['input_ids'])\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vlu07VUshBxE",
        "outputId": "15370cbd-f8ea-4c35-896c-e0d890891469"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'EU',\n",
              " 'rejects',\n",
              " 'German',\n",
              " 'call',\n",
              " 'to',\n",
              " 'boycott',\n",
              " 'British',\n",
              " 'la',\n",
              " '##mb',\n",
              " '.',\n",
              " '[SEP]']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_ids = tokenized_input.word_ids()\n",
        "word_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Kr0TAnfZgdq5",
        "outputId": "98130602-c8b5-4f7c-f49b-5ee2a58c131b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(example_text['ner_tags']), len(tokenized_input[\"input_ids\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9tKISIFRdYEw",
        "outputId": "dc6cdf1f-02a4-441d-b2ee-92a7f70586dc"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function `tokenize_and_align_labels`\n",
        "\n",
        "1.   Set -100 as the label for these special tokens and the subwords we wish to mask during training\n",
        "2.   Mask the subword representations after the first subword\n",
        "3.   Then we align the labels with the token ids using the strategy we picked\n"
      ],
      "metadata": {
        "id": "qVSSABusjaT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_align_labels(example, label_all_tokens = True):\n",
        "  tokenized_input = tokenizer(example['tokens'], truncation=True, is_split_into_words=True)\n",
        "  labels = []\n",
        "\n",
        "  for i, label in enumerate(example['ner_tags']):\n",
        "    word_ids = tokenized_input.word_ids(batch_index=i)\n",
        "    # word_ids(): return a list mapping of the tokens to their actual word in the initial sentence\n",
        "    # It returns a list indicating the word corresponding to each token\n",
        "\n",
        "    previous_word_idx = None\n",
        "    label_ids = []\n",
        "\n",
        "    for word_idx in word_ids:\n",
        "      if word_idx is None:\n",
        "        # set -100 as the label for these special tokens\n",
        "        label_ids.append(-100)\n",
        "      elif word_idx != previous_word_idx:\n",
        "        # if current word_idx is != prev then its the most regular case\n",
        "        # and add the corresponding token\n",
        "        label_ids.append(label[word_idx])\n",
        "      else:\n",
        "        # to take care of sub-words which have the same word_idx\n",
        "        # set -100 as well for them, but only if label_all_tokens == False\n",
        "        label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
        "      previous_word_idx = word_idx\n",
        "    labels.append(label_ids)\n",
        "  tokenized_input['labels'] = labels\n",
        "  return tokenized_input"
      ],
      "metadata": {
        "id": "JQKlWqGtdYB7"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q = tokenize_and_align_labels(conll2003['train'][4:5])\n",
        "\n",
        "print(q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "PYHAw_Lgj25_",
        "outputId": "97615028-f1a0-4b29-b5c4-74d26a059aef"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [[101, 1860, 112, 188, 4702, 1106, 1103, 1735, 1913, 112, 188, 27431, 3914, 14651, 163, 7635, 4119, 1163, 1113, 9031, 11060, 1431, 4417, 8892, 3263, 2980, 1121, 2182, 1168, 1190, 2855, 1235, 1103, 3812, 5566, 1108, 27830, 119, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 5, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, -100]]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Before applying the `tokenize_and_align_labels()` the `tokenized_input` has 3 keys\n",
        "*   `input_ids`\n",
        "*   `token_type_ids`\n",
        "*   `attention_mask`\n",
        "\n",
        "But after applying `tokenize_and_align_labels()` we have an extra key - `labels`"
      ],
      "metadata": {
        "id": "4l9fQhkorjIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token, label in zip(tokenizer.convert_ids_to_tokens(q['input_ids'][0]), q['labels'][0]):\n",
        "  print(f\"{token: <40} {label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5qwFENDcj2dF",
        "outputId": "e00f18b9-6775-4a11-8c07-de467d29415b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS]                                    -100\n",
            "Germany                                  5\n",
            "'                                        0\n",
            "s                                        0\n",
            "representative                           0\n",
            "to                                       0\n",
            "the                                      0\n",
            "European                                 3\n",
            "Union                                    4\n",
            "'                                        0\n",
            "s                                        0\n",
            "veterinary                               0\n",
            "committee                                0\n",
            "Werner                                   1\n",
            "Z                                        2\n",
            "##wing                                   2\n",
            "##mann                                   2\n",
            "said                                     0\n",
            "on                                       0\n",
            "Wednesday                                0\n",
            "consumers                                0\n",
            "should                                   0\n",
            "buy                                      0\n",
            "sheep                                    0\n",
            "##me                                     0\n",
            "##at                                     0\n",
            "from                                     0\n",
            "countries                                0\n",
            "other                                    0\n",
            "than                                     0\n",
            "Britain                                  5\n",
            "until                                    0\n",
            "the                                      0\n",
            "scientific                               0\n",
            "advice                                   0\n",
            "was                                      0\n",
            "clearer                                  0\n",
            ".                                        0\n",
            "[SEP]                                    -100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = conll2003.map(tokenize_and_align_labels, batched=True)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "RBbw9e_TreFJ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForTokenClassification.from_pretrained('bert-base-cased', num_labels=9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "piFfL3wrrd_s",
        "outputId": "3cb93415-13ee-422b-b0a3-fec2f2362bec"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JWuFUIoa7jI",
        "outputId": "d4991bd5-6189-42c4-fa8b-51955342f618"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "args = TrainingArguments(\n",
        "    #\"test-ner\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "collapsed": true,
        "id": "I65XRlHdrd8k",
        "outputId": "cb7b9ced-4cd5-46ac-d3dd-4f97aae2b155"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-49c4352f0af6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m args = TrainingArguments(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m#\"test-ner\",\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mevaluation_strategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForTokenClassification(tokenizer)"
      ],
      "metadata": {
        "id": "5XvhBmdZuAH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate -q # Install the evaluate package which contains load_metric"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3O4afa_pwggl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate  # Import load_metric from evaluate instead of datasets"
      ],
      "metadata": {
        "id": "P1QT-IKYwzIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = evaluate.load('seqeval')\n",
        "\n",
        "example = conll2003['train'][0]\n",
        "\n",
        "label_list = conll2003['train'].features['ner_tags'].feature.names\n",
        "\n",
        "label_list"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gZIkJoxyrd48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [label_list[i] for i in example['ner_tags']]\n",
        "\n",
        "labels"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5vuVtqDCj2Zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric.compute(predictions=[labels], references=[labels])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "dDVQKKp_j2QH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_preds):\n",
        "  pred_logits, labels = eval_preds\n",
        "  pred_logits = np.argmax(pred_logits, axis=-1)\n",
        "\n",
        "  prediction = [\n",
        "      [label_list[eval_preds] for (eval_preds, l) in zip(prediction, label) if l != -100] for prediction, label in zip(pred_logits, labels)\n",
        "  ]\n",
        "  true_labels = [\n",
        "      [label_list[l] for (eval_preds, l) in zip(prediction, label) if l != -100] for prediction, label in zip(pred_logits, labels)\n",
        "  ]\n",
        "  results = metric.compute(predictions=prediction, references=true_labels)\n",
        "\n",
        "  return {\n",
        "      'precision': results['overall_precision'],\n",
        "      'recall': results['overall_recall'],\n",
        "      'f1': results['overall_f1'],\n",
        "      'accuracy': results['overall_accuracy']\n",
        "  }"
      ],
      "metadata": {
        "id": "xOnhFwM50-lB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['validation'],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7NJn_hK30-ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "\n",
        "model.save_pretrained('ner_model')\n",
        "\n",
        "tokenizer.save_pretrained('tokenizer')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7_-y4zBx0-fz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = {\n",
        "    str(i) : label for i, label in enumerate(label_list)\n",
        "}\n",
        "\n",
        "label2id = {\n",
        "    label: str(i) for i, label in enumerate(label_list)\n",
        "}"
      ],
      "metadata": {
        "id": "iCsroZQ24esf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "config = json.load(open('ner_model/config.json'))\n",
        "config['id2label'] = id2label\n",
        "config['label2id'] = label2id\n",
        "\n",
        "json.dump(config, open('ner_model/config.json', 'w'))\n",
        "\n",
        "model_fine_tuned = AutoModelForTokenClassification.from_pretrained('ner_model')"
      ],
      "metadata": {
        "id": "_0lAxESG4ep8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "o3rqybao-VJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "O3sn8g9u0-aY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = pipeline('ner', model=model_fine_tuned, tokenizer=tokenizer)\n",
        "example = \"Bill Gates is the Founder of Microsoft\"\n",
        "\n",
        "ner_results = nlp(example)\n",
        "print(ner_results)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "EiQyMlZb5Wtk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}